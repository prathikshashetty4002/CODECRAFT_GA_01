# ğŸ“Œ Prompt-Based Text Generation | Task 1 â€“ CodeCraft Generative AI Internship

## âœ… Task Overview
This task explores the use of pre-trained language models (specifically GPT-2) to generate coherent text outputs based on given prompts. The objective was to understand how prompt-based generation works in Natural Language Processing (NLP) using Hugging Face's Transformers library.

---

## ğŸ§  Model Used
- ğŸ¤– GPT-2 (`gpt2` model from Hugging Face)

---

## ğŸ’» Technologies & Tools
- Python  
- Google Colab  
- Hugging Face Transformers  
- Google Drive / GitHub for submission

---

## ğŸ” Prompts Used
Some sample prompts used for generation:
- `"Once upon a time in India,"`
- `"Artificial Intelligence will"`
- `"The future of education is"`

---

## ğŸ“‹ Sample Output
Once upon a time in India, the world was in turmoil and the world was experiencing terror...
Artificial Intelligence will do anything. There is no doubt that the future is bright and promising...


---

## ğŸ“ Contents
- `task1.ipynb` â€“ Google Colab notebook used for code execution  
- `task1_report.docx` â€“ Short report summarizing the task and output  
- `screenshot.png` â€“ Screenshot of code and generated text

---

## ğŸ“ Summary
Using a pre-trained GPT-2 model, this task demonstrated how language models can generate text based on user-provided prompts. It showed the power and flexibility of NLP models in generating coherent, context-aware sentences.

---

## ğŸ”— Note
This task was completed as part of the **CodeCraft Generative AI Internship**.
