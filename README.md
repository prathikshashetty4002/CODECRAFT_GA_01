# 📌 Prompt-Based Text Generation | Task 1 – CodeCraft Generative AI Internship

## ✅ Task Overview
This task explores the use of pre-trained language models (specifically GPT-2) to generate coherent text outputs based on given prompts. The objective was to understand how prompt-based generation works in Natural Language Processing (NLP) using Hugging Face's Transformers library.

---

## 🧠 Model Used
- 🤖 GPT-2 (`gpt2` model from Hugging Face)

---

## 💻 Technologies & Tools
- Python  
- Google Colab  
- Hugging Face Transformers  
- Google Drive / GitHub for submission

---

## 🔍 Prompts Used
Some sample prompts used for generation:
- `"Once upon a time in India,"`
- `"Artificial Intelligence will"`
- `"The future of education is"`

---

## 📋 Sample Output
Once upon a time in India, the world was in turmoil and the world was experiencing terror...
Artificial Intelligence will do anything. There is no doubt that the future is bright and promising...


---

## 📁 Contents
- `task1.ipynb` – Google Colab notebook used for code execution  
- `task1_report.docx` – Short report summarizing the task and output  
- `screenshot.png` – Screenshot of code and generated text

---

## 📝 Summary
Using a pre-trained GPT-2 model, this task demonstrated how language models can generate text based on user-provided prompts. It showed the power and flexibility of NLP models in generating coherent, context-aware sentences.

---

## 🔗 Note
This task was completed as part of the **CodeCraft Generative AI Internship**.
